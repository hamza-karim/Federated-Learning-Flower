{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'V1_23_Feb_180m_anomalous_data(gaussian).csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m tf.random.set_seed(\u001b[32m42\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m data1 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mV1_23_Feb_180m_anomalous_data(gaussian).csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m dataframe = data1\n\u001b[32m     18\u001b[39m dataframe[\u001b[33m'\u001b[39m\u001b[33mdatetimestamp\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(dataframe[\u001b[33m'\u001b[39m\u001b[33mdatetimestamp\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.karim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.karim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.karim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.karim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\muhammad.karim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'V1_23_Feb_180m_anomalous_data(gaussian).csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "data1 = pd.read_csv('V1_23_Feb_180m_anomalous_data(gaussian).csv')\n",
    "dataframe = data1\n",
    "\n",
    "dataframe['datetimestamp'] = pd.to_datetime(dataframe['datetimestamp'])\n",
    "\n",
    "# Take only 'datetimestamp', 'Hz_mod_anomaly', and 'mod_BIN' columns\n",
    "df = dataframe[['datetimestamp', 'Hz_mod_anomaly', 'mod_BIN']]\n",
    "df.set_index('datetimestamp', inplace=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "plt.plot(df.index, df['Hz_mod_anomaly'], label='Hz_mod_anomaly', color='blue')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.gcf().set_facecolor('white')\n",
    "plt.xlabel('Date Timestamp')\n",
    "plt.ylabel('Hz Mod Anomaly')\n",
    "plt.title('Anomalous Hz Over Time')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "703643f4f39749d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# print start and end date\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mstart date is:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mdf\u001b[49m.index.min())\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mend date is:\u001b[39m\u001b[33m\"\u001b[39m, df.index.max())\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Now we have to choose train and test sets \u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# We will train LSTM autoencoders using only normal dataset which is labeled as 0 in the dataframe we have to separate normal from anomalous data.\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Separate normal data (label 0) from anomalous data\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# print start and end date\n",
    "print(\"start date is:\", df.index.min())\n",
    "print(\"end date is:\", df.index.max())\n",
    "# Now we have to choose train and test sets \n",
    "# We will train LSTM autoencoders using only normal dataset which is labeled as 0 in the dataframe we have to separate normal from anomalous data.\n",
    "\n",
    "# Separate normal data (label 0) from anomalous data\n",
    "normal_data = df[df['mod_BIN'] == 0]\n",
    "anomalous_data = df[df['mod_BIN'] != 0]\n",
    "\n",
    "# Print start and end dates of normal data\n",
    "print(\"Start date of normal data is:\", normal_data.index.min())\n",
    "print(\"End date of normal data is:\", normal_data.index.max())\n",
    "\n",
    "# Print start and end dates of anomalous data\n",
    "print(\"Start date of anomalous data is:\", anomalous_data.index.min())\n",
    "print(\"End date of anomalous data is:\", anomalous_data.index.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a68d498a95444",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we make train and test dataset\n",
    "# in this case 80 % of normal data would be used for training and rest 20% of normal data + all anomalous data would be used in test \n",
    "\n",
    "start_anomalous_date = anomalous_data.index.min()\n",
    "\n",
    "# Split normal data into train before anomaly and train after anomaly\n",
    "train_before_anomaly = normal_data[normal_data.index < start_anomalous_date]\n",
    "train_after_anomaly = normal_data[normal_data.index >= start_anomalous_date]\n",
    "\n",
    "# Calculate the number of rows for 80% of normal data\n",
    "train_normal_size = int(0.8 * len(normal_data))\n",
    "\n",
    "# Select the first 80% of normal data for training\n",
    "train = normal_data.iloc[:train_normal_size]\n",
    "\n",
    "# Select the remaining 20% of normal data for testing\n",
    "test_normal = normal_data.iloc[train_normal_size:]\n",
    "\n",
    "# Concatenate test normal data with anomalous data\n",
    "test = pd.concat([test_normal, anomalous_data])\n",
    "\n",
    "print(\"Shape of train:\", train.shape)\n",
    "print(\"Shape of test:\", test.shape)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size if needed\n",
    "sns.lineplot(x='datetimestamp', y='Hz_mod_anomaly', data=train_before_anomaly, label='Train Data', color='blue')\n",
    "sns.lineplot(x='datetimestamp', y='Hz_mod_anomaly', data=train_after_anomaly, color='blue')\n",
    "sns.lineplot(x='datetimestamp', y='Hz_mod_anomaly', data=test_normal, color='red')\n",
    "sns.lineplot(x='datetimestamp', y='Hz_mod_anomaly', data=anomalous_data, label='Test Data', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Hz_mod_anomaly')\n",
    "plt.title('Train vs Test Data')\n",
    "plt.legend()\n",
    "plt.gca().set_facecolor('white')\n",
    "plt.gcf().set_facecolor('white')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85102876a9194a0d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Since LSTM use sigmoid and tanh so we need out values to be normalized , we will use Standard Scaler for that\n",
    "# \n",
    "# scaler = StandardScaler()\n",
    "# scaler = scaler.fit(train[['Hz_mod_anomaly']])\n",
    "# \n",
    "# train.loc[:, 'Hz_mod_anomaly'] = scaler.transform(train[['Hz_mod_anomaly']])\n",
    "# test.loc[:, 'Hz_mod_anomaly'] = scaler.transform(test[['Hz_mod_anomaly']])\n",
    "\n",
    "# Drop the 'mod_BIN' column from train and test DataFrames because it was just labels to split between train and test \n",
    "train = train.drop(columns=['mod_BIN'])\n",
    "test = test.drop(columns=['mod_BIN'])\n",
    "\n",
    "# Display the first few rows of train and test sets\n",
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc78bc2d25f4a6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_size = 20 # Number of time steps to look back \n",
    "# larger sequence size (look further back) may improve forecasting \n",
    "\n",
    "def to_sequence(x, y, seq_size=1):\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    \n",
    "    for i in range(len(x)-seq_size):\n",
    "        #print(i)\n",
    "        x_values.append(x.iloc[i:(i+seq_size)].values)\n",
    "        y_values.append(y.iloc[i+seq_size])\n",
    "        \n",
    "    return np.array(x_values), np.array(y_values)\n",
    "\n",
    "trainX, trainY = to_sequence(train[['Hz_mod_anomaly']], train['Hz_mod_anomaly'], seq_size)\n",
    "testX, testY = to_sequence(test[['Hz_mod_anomaly']], test['Hz_mod_anomaly'], seq_size)\n",
    "\n",
    "print(\"train X shape\", trainX.shape)\n",
    "print(\"train Y shape\", trainY.shape)\n",
    "print(\"test X shape\", testX.shape)\n",
    "print(\"test Y shape\", testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11468c30890134",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed, Dense\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, activation='tanh', recurrent_activation='sigmoid', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "model.add(LSTM(64, activation='tanh', recurrent_activation='sigmoid', return_sequences=False))\n",
    "model.add(RepeatVector(trainX.shape[1]))\n",
    "model.add(LSTM(64, activation='tanh', recurrent_activation='sigmoid', return_sequences=True))\n",
    "model.add(LSTM(128, activation='tanh', recurrent_activation='sigmoid', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(trainX.shape[2])))\n",
    "model.compile(optimizer='adam', loss='mae', metrics=[\"mape\"])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mae', metrics=[\"mape\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6bd6a232abcd4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Measure the time\n",
    "start_training_time = time.time()\n",
    "# Fit the model\n",
    "history = model.fit(trainX, trainY, epochs=35, batch_size=100, validation_split=0.2, verbose=1)\n",
    "end_training_time = time.time()\n",
    "training_time = end_training_time - start_training_time\n",
    "print(f\"Total training time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d2254c845893cd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training history\n",
    "training_loss = history.history['loss']\n",
    "training_mape = history.history['mape']\n",
    "val_loss = history.history['val_loss']\n",
    "val_mape = history.history['val_mape']\n",
    "epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot training and validation loss \n",
    "plt.plot(epochs, training_loss, color='blue', label='Training loss')\n",
    "plt.plot(epochs, val_loss, color='green', label='Validation loss')\n",
    "\n",
    "# Plot training and validation MAPE \n",
    "plt.plot(epochs, training_mape, color='orange', linestyle='--', label='Training MAPE')\n",
    "plt.plot(epochs, val_mape, color='red', linestyle='--', label='Validation MAPE')\n",
    "\n",
    "plt.title('Training and Validation Loss / MAPE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss / MAPE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Extract the final loss and MAPE values\n",
    "final_training_loss = training_loss[-1]\n",
    "final_val_loss = val_loss[-1]\n",
    "final_training_mape = training_mape[-1]\n",
    "final_val_mape = val_mape[-1]\n",
    "\n",
    "# Print the final loss and MAPE values\n",
    "print(\"Final Training Loss:\", final_training_loss)\n",
    "print(\"Final Validation Loss:\", final_val_loss)\n",
    "print(\"Final Training MAPE:\", final_training_mape)\n",
    "print(\"Final Validation MAPE:\", final_val_mape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca894b02ed77ab0d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# When reconstruction error (MAPE) is larger than the threshold which we set then there is anomaly\n",
    "\n",
    "# Calculate MAE for training prediction\n",
    "trainPredict = model.predict(trainX)\n",
    "trainMAE = np.mean(np.abs(trainPredict - trainX), axis=1)\n",
    "# Print the mean of test MAE\n",
    "print(\"Mean of Train MAE:\", np.mean(trainMAE))\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(trainMAE, bins=30)\n",
    "plt.xlabel('Mean Absolute Error (MAE)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Mean Absolute Error (MAE) in Training Prediction')\n",
    "plt.show()\n",
    "\n",
    "# Calculate MAPE for each sample\n",
    "trainActual = trainX  # Assuming trainX contains the actual values\n",
    "trainMAPE = np.mean(np.abs(trainPredict - trainActual) / trainActual, axis=1) * 100\n",
    "\n",
    "# Print the mean of MAPE\n",
    "print(\"Mean of Train MAPE:\", np.mean(trainMAPE))\n",
    "\n",
    "# Plot histogram of MAPE\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(trainMAPE, bins=30)\n",
    "plt.xlabel('Mean Absolute Percentage Error (MAPE)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Mean Absolute Percentage Error (MAPE) in Training Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b0dcf7d28c729",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate reconstruction loss (MAE) for testing dataset\n",
    "testPredict = model.predict(testX)\n",
    "testMAE = np.mean(np.abs(testPredict - testX), axis=1)\n",
    "\n",
    "# Print the mean of test MAE\n",
    "print(\"Mean of Test MAE:\", np.mean(testMAE))\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(testMAE, bins=30)\n",
    "plt.xlabel('Test MAE')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Test MAE')\n",
    "plt.show()\n",
    "\n",
    "# Calculate MAPE for each sample\n",
    "testActual = testX  # Assuming trainX contains the actual values\n",
    "testMAPE = np.mean(np.abs(testPredict - testActual) / testActual, axis=1) * 100\n",
    "\n",
    "# Print the mean of MAPE\n",
    "print(\"Mean of Test MAPE:\", np.mean(testMAPE))\n",
    "\n",
    "# Plot histogram of MAPE\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(testMAPE, bins=30)\n",
    "plt.xlabel('Mean Absolute Percentage Error (MAPE)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Mean Absolute Percentage Error (MAPE) in test Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b39d4e0c3a1ca2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_trainMAE = 0.76\n",
    "max_trainMAPE = 0.1\n",
    "# thresholding using MAPE \n",
    "anomaly_df = pd.DataFrame(test[seq_size:])\n",
    "anomaly_df['testMAPE'] = testMAPE\n",
    "anomaly_df['max_trainMAPE'] = max_trainMAPE\n",
    "anomaly_df['anomaly'] = anomaly_df['testMAPE'] > anomaly_df['max_trainMAPE']\n",
    "anomaly_df['Hz_mod_anomaly'] = test[seq_size:]['Hz_mod_anomaly']\n",
    "\n",
    "# Plot test MAE vs max_trainMAE\n",
    "sns.lineplot(x=anomaly_df.index, y=anomaly_df['testMAPE'])\n",
    "sns.lineplot(x=anomaly_df.index, y=anomaly_df['max_trainMAPE'])\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc92a8df8ddcbf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract anomalies DataFrame with 'anomaly' column equal to True\n",
    "anomalies = anomaly_df.loc[anomaly_df['anomaly'] == True]\n",
    "# \n",
    "# # Reshape the 'Hz_mod_anomaly' column to be a 2-dimensional array\n",
    "# hz_mod_anomaly_reshaped = scaler.inverse_transform(anomaly_df['Hz_mod_anomaly'].values.reshape(-1, 1))\n",
    "# \n",
    "# # Reshape the anomalies 'Hz_mod_anomaly' column to be a 2-dimensional array\n",
    "# anomalies_reshaped = scaler.inverse_transform(anomalies['Hz_mod_anomaly'].values.reshape(-1, 1))\n",
    "# \n",
    "# # Plot the data\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.lineplot(x=anomaly_df.index, y=hz_mod_anomaly_reshaped.flatten(), color='blue')  # Flatten to convert 2D array to 1D array\n",
    "# sns.scatterplot(x=anomalies.index, y=anomalies_reshaped.flatten(), color='red')  # Flatten to convert 2D array to 1D array\n",
    "# \n",
    "# # Show plot\n",
    "# plt.show()\n",
    "\n",
    "# Reshape the 'Hz_mod_anomaly' column to be a 2-dimensional array\n",
    "hz_mod_anomaly_reshaped = anomaly_df['Hz_mod_anomaly'].values.reshape(-1, 1)\n",
    "\n",
    "# Reshape the anomalies 'Hz_mod_anomaly' column to be a 2-dimensional array\n",
    "anomalies_reshaped = anomalies['Hz_mod_anomaly'].values.reshape(-1, 1)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=anomaly_df.index, y=hz_mod_anomaly_reshaped.flatten(), color='blue') \n",
    "sns.scatterplot(x=anomalies.index, y=anomalies_reshaped.flatten(), color='red')  \n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8472ca84f80ed38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_percentage_error, confusion_matrix\n",
    "\n",
    "# Load the dataset(s)\n",
    "# Adjust the path to your dataset file\n",
    "data1 = pd.read_csv('V3S3.csv')  # Replace 'V3S1.csv' with the correct file path if needed\n",
    "\n",
    "# Combine the datasets (if you have more)\n",
    "combined_data = pd.concat([data1], ignore_index=True)\n",
    "\n",
    "# Preprocess the combined dataset\n",
    "combined_data['datetimestamp'] = pd.to_datetime(combined_data['datetimestamp'])\n",
    "\n",
    "# Plot the combined dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='datetimestamp', y='Hz_mod_anomaly', data=combined_data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.gcf().set_facecolor('white') \n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.title('Testing Dataset Version 3 Signature 1')\n",
    "plt.show()\n",
    "\n",
    "# Convert the combined dataset into sequences\n",
    "combined_X, combined_Y = to_sequence(combined_data[['Hz_mod_anomaly']], combined_data['Hz_mod_anomaly'], seq_size)\n",
    "\n",
    "# Measure the time before starting the prediction\n",
    "start_time = time.time()\n",
    "\n",
    "# Use the trained model to predict the reconstruction errors (MAPE) on the combined dataset\n",
    "combined_predict = model.predict(combined_X)\n",
    "\n",
    "# Measure the time after prediction\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total inference time\n",
    "inference_time = end_time - start_time\n",
    "print(f\"Total inference time: {inference_time:.2f} seconds\")\n",
    "\n",
    "# Calculate combined MAPE\n",
    "combined_mape = np.mean(np.abs(combined_predict - combined_X) / combined_X, axis=1) * 100\n",
    "\n",
    "# Thresholding using MAPE\n",
    "max_trainMAPE = 0.1\n",
    "\n",
    "# Capture all details in a DataFrame for easy plotting\n",
    "anomaly_df = pd.DataFrame(combined_data[seq_size:])\n",
    "anomaly_df['combinedMAPE'] = combined_mape\n",
    "anomaly_df['max_trainMAPE'] = max_trainMAPE\n",
    "anomaly_df['anomaly'] = anomaly_df['combinedMAPE'] > max_trainMAPE\n",
    "anomaly_df['Hz_mod_anomaly'] = combined_data[seq_size:]['Hz_mod_anomaly']\n",
    "\n",
    "# Plot combined MAPE vs max_trainMAPE\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=anomaly_df.index, y=anomaly_df['combinedMAPE'], label='MAPE')\n",
    "sns.lineplot(x=anomaly_df.index, y=anomaly_df['max_trainMAPE'], label='Threshold', linestyle='--')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean Absolute Percentage Error (MAPE)')\n",
    "plt.title('Anomaly Detection on Testing Dataset (V3-S2)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Identify anomalies based on the threshold\n",
    "combined_anomalies = anomaly_df[anomaly_df['anomaly'] == True]\n",
    "\n",
    "# Plot anomalies\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=combined_data['datetimestamp'], y=combined_data['Hz_mod_anomaly'], color='blue', label='Normal Data')\n",
    "sns.scatterplot(x=combined_anomalies['datetimestamp'], y=combined_anomalies['Hz_mod_anomaly'], color='red', label='Anomalies')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.title('Anomaly Detection on Testing Dataset (V3-S2)')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Get the true labels\n",
    "true_labels = (combined_data['mod_BIN'][seq_size:] != 0).astype(int)\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, combined_mape > max_trainMAPE)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "TP = conf_matrix[1, 1]\n",
    "FP = conf_matrix[0, 1]\n",
    "TN = conf_matrix[0, 0]\n",
    "FN = conf_matrix[1, 0]\n",
    "\n",
    "# Print the counts\n",
    "print(\"True Positives (Anomalies correctly predicted as anomalies):\", TP)\n",
    "print(\"False Positives (Normal data incorrectly predicted as anomalies):\", FP)\n",
    "print(\"True Negatives (Normal data correctly predicted as normal):\", TN)\n",
    "print(\"False Negatives (Anomalies incorrectly predicted as normal):\", FN)\n",
    "\n",
    "# Calculate Precision, Recall, and F1-score\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n",
    "accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(f\"Total inference time: {inference_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
